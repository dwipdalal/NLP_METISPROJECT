# -*- coding: utf-8 -*-
"""Final code

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10dSdPxIzs6xh6X4HfOShKsaxcYTpVA7f
"""

# importing certain libraries that shall be usefull for reading and excecuting the code
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.preprocessing.text import Tokenizer

sentiments = pd.read_csv('https://raw.githubusercontent.com/dwipdalal/NLP_METISPROJECT/main/sentiment_labels.txt', delimiter='|')
sentiments.head()
# this is the dataset of sentiment values corresponding to the phrase id.

dataset_phrases = pd.read_csv('https://raw.githubusercontent.com/dwipdalal/NLP_METISPROJECT/main/dictionary.txt',delimiter='|')
#this is dataset of phrases 
# since for getting a good look of overall dataset it is important to put phrase and there sentiment values in a single column
# the phrase in dataset_phrase are jumbled so for concating two dataset that is dataset_phrase and sentiments it is necessary to make the phrase id column of both in same order so we shall sort the dataset_phrase
dataset_phrases.sort_values('0',inplace = True)
dataset_phrases.set_index('0', inplace=True)
dataset_phrases.head()

df = pd.concat([dataset_phrases,sentiments], axis=1)
df.head()
df.shape
df.tail()

#missing value remover
df.isnull()
df.dropna(how = 'any', axis = 0, inplace = True)
df.shape
# this shows that there are no null values

# since the sentiment values are fraction number and  since we have to perform 3 class classifcation we have divide fractional values into 3 classes
# 0 - [0, 0.333]
# 1 - [0.333, 0.666]
# 2 - [0.333, 0.666]
p = []
a = 0.333
b = 0.666
for i in list(df['sentiment values']):
  if (i>=0 and i<=a): 
    p.append(0)
  elif i>a and i<=b:
    p.append(1)
  else:
    p.append(2)
len(p)

one = p.count(1)
zero = p.count(0)
two = p.count(2)
fig = plt.figure(figsize =(10, 7))
plt.pie([zero,one,two], labels = [0,1,2])
plt.show()

# to split dataset into training and testing part
X = df['!'].astype('str')
y = p
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y,stratify = y,test_size=0.1, random_state=42)
# 10% of the original dataset shall be used for testing purposes 
# stratify locks the distribution of different class labels. I.e. ratios of distribution of class labels in train and test will be almost same

# this is for tokenizing the words, then each word of the tokenized list shall be given a specific index 
tokenizer = Tokenizer(oov_token= '<0VV>') # <0VV> is the token that the model shall replace the unseen words with. So if some word of the test case is not there in tokenized list then that word shall be replaces with 0VV this is done to maintain the size of the sentence. 
tokenizer.fit_on_texts(X_train.astype(str)) # we have to use astype(str) as there are some numbers in df[!]
word_index_ = tokenizer.word_index # gives index to each word and then makes a dictionary of word and their indexes.


# since computer does not understand letters so we shall make list of sequence of numbers that is replaces sentences with sequences of indexes corresponding to those words that shall be feeded to our neural network model.
# the numbers that would constitue to this list are those numbers which are index of the corresponding words in the dictionary.
sequence_train = tokenizer.texts_to_sequences(X_train)
sequence_test = tokenizer.texts_to_sequences(X_test)

# different sentences has different length so we need to pad the sentences inorder to make all of them of same length before feeding them in our model
from tensorflow.keras.preprocessing.sequence import pad_sequences
padded_train = pad_sequences(sequence_train, maxlen= 52)
padded_test = pad_sequences(sequence_test, maxlen= 52)

type(padded_test)

X_train = padded_train
X_test = padded_test
y_train = tf.keras.utils.to_categorical(y_train,num_classes= 3)
y_train

"""# Early stopping
To prevent overfitting of the model we shall be using early stopping. The corresponding callback function shall stop training when the validation accuracy does not increase for 3 consecutive epochs 
"""

callback= tf.keras.callbacks.EarlyStopping(monitor="val_accuracy",patience=3, verbose=2, mode="auto", restore_best_weights=True)
# patience counts number of epochs with no developement and then stops the epochs

# list to append all of the three models
a = []

X_train, X_val, y_train, y_val = train_test_split(X_train, y_train,stratify = y_train ,test_size=0.1)

"""### Model 1

The sentences until now are just sequences of words and have no real meaning therefore we shall do word embedding to make sematic vectors. These vectors will be made by using the concept: meaning of the words is know by the company it has.
"""

model1 = keras.models.Sequential()
model1.add(keras.layers.Embedding(18079+1, 52, input_length=52))
from keras.layers import LSTM
model1.add(LSTM(100))
model1.add(keras.layers.Dense(10, activation="relu"))
model1.add(keras.layers.Dense(3, activation="softmax"))

model1.summary()

model1.compile(loss= "categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

"""By setting verbose 0, 1 or 2 you just say how do you want to 'see' the training progress for each epoch.

verbose=0 will show you nothing (silent)

verbose=1 will show you an animated progress bar 

verbose=2 will just mention the number of epoch like this:
"""

y_val.shape

#default batch size is 32
history = model1.fit(X_train, y_train, epochs = 8, verbose=2, validation_data=(X_val,y_val), callbacks = [callback]) # the validation data should be provied as tuple or less there is a huge error in format matching

y_test = tf.keras.utils.to_categorical(y_test, num_classes= 3)
print(y_test.shape, X_test.shape)

# to plot the graph of accuracy vs epochs
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], 'b')
plt.plot(history.history['val_accuracy'], 'r')
plt.title('Model Accuracy'),
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

"""Since now our model is trianed so we shall find it's actuall accuracy. For doing that we shall pass unseen data into the model and then check the corresponing accuracy. The unseen data in this case is the test data. """

scores = model1.evaluate(X_test, y_test, verbose=0)
print('Test accuracy:', scores[1])

model1.save('model1.h5')

a.append(model1)

"""### Model2"""

from keras.layers import LSTM
from keras.layers import Bidirectional, Dense
model2 = keras.models.Sequential()
model2.add(keras.layers.Embedding(18079+1, 52, input_length=52))
model2.add(Bidirectional(LSTM(52)))
model2.add(keras.layers.Dense(10, activation="relu"))
model2.add(keras.layers.Dense(3, activation="softmax"))

model2.summary()

model2.compile(loss= "categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

print(X_train.shape, y_train.shape)

x = model2.fit(X_train, y_train, epochs = 8, verbose=2, validation_data=(X_val,y_val), callbacks = [callback])
#val_accuracy = model.

print(X_test.shape,y_test.shape)

scores = model2.evaluate(X_test, y_test, verbose=2)
print('Test accuracy:', scores[1])

history = x
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], 'b')
plt.plot(history.history['val_accuracy'], 'r')
plt.title('Model Accuracy'),
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

model2.save('model2.h5')

a.append(model2)

"""### Model3"""

model3 = keras.models.Sequential()
model3.add(keras.layers.Embedding(18079+1, 52, input_length=52))
model3.add(keras.layers.GlobalAveragePooling1D())
model3.add(keras.layers.Dense(30, activation="relu"))
model3.add(keras.layers.Dense(10, activation="relu"))
model3.add(keras.layers.Dense(3, activation="softmax"))

model3.summary()

model3.compile(loss= "categorical_crossentropy", optimizer="adam", metrics=["accuracy"])

print(X_train.shape, y_train.shape)

x = model3.fit(X_train, y_train, epochs = 8, verbose=2, validation_data=(X_val,y_val), callbacks = [callback])

model3.layers[0].name

print(X_test.shape,y_test.shape)

scores = model3.evaluate(X_test, y_test, verbose=2)
print('Test accuracy:', scores[1])

history = x
import matplotlib.pyplot as plt
plt.plot(history.history['accuracy'], 'b')
plt.plot(history.history['val_accuracy'], 'r')
plt.title('Model Accuracy'),
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'], loc='upper left')
plt.show()

model3.save('model3.h5')

a.append(model3)

"""# Integrated Stacking 
In this case we shall be using neural networks as meta-learner for three sub-models that is LSTM, Feed forward neural network and bidirection LSTM. 
"""

from tensorflow.keras.layers import Concatenate
from tensorflow.keras.utils import plot_model

def stacked_model(members):
	# update all layers in all models to not be trainable
  #print(members)
  for i in range(len(members)):
    model = members[i]
    for layer in model.layers:
      layer.trainable = False
			
	# define multi-headed input
  ensemble_visible = [model.input for model in members]
  # concatenate merge output from each model
  ensemble_outputs = [model.output for model in members]
  merge = Concatenate()(ensemble_outputs)
  #print(merge, type(merge))
  
  hidden = keras.layers.Dense(7, activation='relu')(merge)
  output = keras.layers.Dense(3, activation='softmax')(hidden)
  model = tf.keras.Model(inputs=ensemble_visible, outputs=output)
  plot_model(model, show_shapes=True, to_file='model.png')
	# compile
  model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
  return model

a

stacked_model = stacked_model(a)
stacked_model

stacked_model.input

X_val

y_val.shape

type(stacked_model)

pal = [X_val,X_val,X_val]

histoy = stacked_model.fit(pal, y_val,epochs = 3, verbose = 2)

pala = [X_test,X_test,X_test]

scoreeee = stacked_model.evaluate(pala,y_test,verbose = 2)
print(scoreeee[1])

stacked_model.save('bangon.h5')